\documentclass{article}

%-------------------------------------------------

\usepackage{fullpage}
\usepackage{setspace}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{color}

%------------------------

%-------------------------------------------------
\begin{document}

\title{
Upon constraining the spatial distribution of transient gravitational-wave sources
}

\author{
Reed Essick
}

\maketitle

%------------------------

\doublespace

We begin by asserting that graviational waves (GWs) are interesting and therefore their sources are interesting.
Furthermore, we assert that the spacetime distribution of sources is also interesting and set out to measure that distribution using transient GW signals.
A bit of nomenclature

\vspace{0.5cm}
\begin{tabular}{p{3cm}p{12cm}}
    $R(m_1, m_2, z, \Omega)$ & the rate of signals from a direction in the sky ($\Omega$) at redshift $z$ with masses $m_1$, $m_2$, etc, measured with respect to detector-frame time. \\
    $dV/d\Omega dz$     & the differntial sensitive volume observed by our detectors as a function of time, position on the sky, and $z$, which depends on the detector sensitivity. \\
    $\mathcal{F}(m_1,m_2,z,\Omega;t)$ & the probability of detecting a signal with these parameters at this time (related to probabilities of specific noise realizations and the associated signal-to-noise ratios); in most cases this may be treated approximately as a step function.
\end{tabular}

\vspace{0.5cm}
\noindent
We also assume that reasonable priors for our Bayesian inference on a per-event basis follow the modeled astrophysical distribution so that
\begin{equation}
    \frac{p(m_1, m_2, z, \Omega|t,R)}{p(m_1^\prime, m_2^\prime, z^\prime, \Omega^\prime|t^\prime,R)} = \frac{R(m_1, m_2, z, \Omega) \left.(dV_c/d\Omega dz)\right|_{\Omega, z} \mathcal{F}(m_1, m_2, z, \Omega; t)}{R(m_1^\prime, m_2^\prime, z^\prime, \Omega^\prime) \left.(dV_c/d\Omega dz)\right|_{\Omega^\prime, z^\prime} \mathcal{F}(m_1^\prime, m_2^\prime, z^\prime, \Omega^\prime; t^\prime)}
\end{equation}
This means that the expected number of signals with masses $m_1$, $m_2$, etc. coming from $\Omega$ at $z$ coincident with $t$ will be something like 
\begin{equation}
    N_\mathrm{astro} = \left[ R(m_1, m_2, \Omega, z) \frac{dV_c}{d\Omega dz} \Delta\Omega \Delta z \Delta m_1 \Delta m_2 \right] \Delta t
\end{equation}
and we expect to detect 
\begin{equation}
    N_\mathrm{detect} = N_\mathrm{astro} \mathcal{F}(m_1, m_2, z, \Omega; t)
\end{equation}

Now, we consider rates that are low enough so that only one type of signal ($m_1$, $m_2$, $z$, $\Omega$) is detectable at a time and further model each type of signal as an independent Poisson process.
Approximating this as a set of discrete bins indexed by $k$, we expect
\begin{align}
    p(\mathrm{data}|\mathrm{signal}; R, t)p(\mathrm{signal}|R, t) & = \sum_k p(n_k=1|R_k dV_k \mathcal{F}_k \Delta t) \prod\limits_{j \neq k} p(n_j=0|R_j dV_j \mathcal{F}_j \Delta t) p\left(\mathrm{data}|m_1^{(k)}, m_2^{(k)}, \Omega^{(k)}, z^{(k)}\right) \nonumber \\
                                                                  & = \sum_k \left(R_k dV_k \mathcal{F}_k \Delta t\right) e^{-R_k dV_k \mathcal{F}_k \Delta t} \prod\limits_{j \neq k} e^{-R_j dV_j \mathcal{F}_j \Delta t} p\left(\mathrm{data}|m_1^{(k)}, m_2^{(k)}, \Omega^{(k)}, z^{(k)}\right) \nonumber \\
                                                                  & = \left[\sum_k \left(R_k dV_k \mathcal{F}_k \Delta t\right) p\left(\mathrm{data}|m_1^{(k)}, m_2^{(k)}, \Omega^{(k)}, z^{(k)}\right) \right] e^{-\sum\limits_j R_j dV_j \mathcal{F}_j \Delta t} \nonumber \\
                                                                  & = \Delta t \left[ \int dm_1 dm_2 dz d\Omega \left(R \frac{dV_c}{d\Omega dz} \mathcal{F}\right) p\left(\mathrm{data}|m_1, m_2, \Omega, z\right) \right] e^{-\left[\int dm_1 dm_2 dz d\Omega \left(R \frac{dV_c}{d\Omega dz} \mathcal{F}\right)\right] \Delta t}
\end{align}
and similarly
\begin{align}
    p(\mathrm{data}|\mathrm{noise}; R, t)p(\mathrm{noise}|R, t) & = p(\mathrm{data}|\mathrm{noise}) \prod\limits_j p(n_j=0|R_jdV_j \mathcal{F}_j \Delta t) \\
                                                                & = p(\mathrm{data}|\mathrm{noise}) e^{-\left[\int dm_1 dm_2 dz d\Omega \left(R \frac{dV_c}{d\Omega dz} \mathcal{F}\right)\right] \Delta t} \\
\end{align}
These define the relevant likelihoods for each small time segment.
We assume many time segments are independent and then stack everything together.
This give an overall joint likelihood of observing the set of detections we actually observed, conditioned on a Rate distribution: $R(m_1, m_2, z, \Omega)$.
Following Messenger+Veitch (2013), we obtain
\begin{align}
    p(\mathrm{data}|R) = & \prod\limits_{i|d_i\geq\mathrm{thr}} \left[ p(d_i|\mathrm{noise}) + \int dm_1 dm_2 dz d\Omega p(d_i|m_1, m_2, z, \Omega) R\frac{dV_c}{d\Omega dz} \mathcal{F}_i \Delta t \right] e^{-\int dm_1 dm_2 dz d\Omega R\frac{dV_c}{d\Omega dz} \mathcal{F}_i \Delta t} \nonumber \\
                         & \times \prod\limits_{j|d_j<\mathrm{thr}} \left[ p(d_i<\mathrm{thr}|\mathrm{noise}) + \int dm_1 dm_2 dz d\Omega p(d_i<\mathrm{thr}|m_1, m_2, z, \Omega) R\frac{dV_c}{d\Omega dz} \mathcal{F}_j \Delta t \right] e^{-\int dm_1 dm_2 dz d\Omega R\frac{dV_c}{d\Omega dz} \mathcal{F}_j \Delta t}
\end{align}
which we simplify by assuming
\begin{align}
    p(d_i<\mathrm{thr}|\mathrm{noise}) & = 1 \\
    p(d_i<\mathrm{thr}|m_1, m_2, z, \Omega)\mathcal{F}_i & = 0
\end{align}
This is equivalent to normalizing the likelihood appropriately to assume we only see signals above our threshold. 
In reality, we should be able to extract more information by lowering the threshold to include marginal events, but the associated benefits are expected to be realtively small with a significant computational overhead.
With our simplifying assumptions, we obtain
\begin{multline}
    \log p(\mathrm{data}|R) = - \int dt dm_1 dm_2 dz d\Omega \left(R \frac{dV_c}{d\Omega dz} \mathcal{F}\right) \\
         + \sum\limits_{i|d_i\geq\mathrm{thr}} \log \left[ \Delta t \int dm_1 dm_2 dz d\Omega \left(R \frac{dV_c}{d\Omega dz} \mathcal{F}\right) p(d_i|m_1, m_2, \Omega, z) \right] \\
         + \text{terms independent of }R
\end{multline}
This looks something like a Poisson factor for the overall rate along with the inner product of the evidence for each individual event.
We can use this marginal likelihood for the Rate distribution to make inferences about the rate.
Several approaches for sampling $p(R|\mathrm{data})$ have been proposed
\begin{itemize}
    \item bin the sky into separate voxels and run an MCMC direcly over the rate in each voxel. This can be expensive if there are a lot of voxels.
    \item assume the deviations from isotropy are small and attempt to infer the posterior of the deviations using Gaussian processes (see below).
\end{itemize}
along with several sanity checks and/or back-of-the-envelop estimates
\begin{itemize}
    \item divide the sky into hemispheres and count the posterior weight in each hemisphere as a statistic looking for a dipole.
    \item compute the 2-point auto-correlation function with stacked posteriors to look for preferred length scales.
\end{itemize}

%------------------------

\subsection*{Marginal likelihood for $R(\Omega)$}

\textcolor{red}{We note that we're primarily interested in $R(\Omega)$ at the moment and assume independence from ($m_1$, $m_2$, $z$) and marginalize those away.}

%------------------------

\subsection*{Guassian process regression of anisotropies}

\textcolor{red}{write this up. The key thing is to expand the log of the evidence in a Taylor expansion which makes the logLikelihood linear in the rate distribution, which yields an analytically tractable posterior with a Gaussian Process prior}

%-------------------------------------------------
\end{document}
