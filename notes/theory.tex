\documentclass{article}

%-------------------------------------------------

\usepackage{fullpage}
\usepackage{setspace}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{color}

%------------------------

%-------------------------------------------------
\begin{document}

\title{
Upon constraining the spatial distribution of transient gravitational-wave sources
}

\author{
Reed Essick
}

\maketitle

%------------------------

\doublespace

We begin by asserting that graviational waves (GWs) are interesting and therefore their sources are interesting.
Furthermore, we assert that the spacetime distribution of sources is also interesting and set out to measure that distribution using transient GW signals.
A bit of nomenclature

\vspace{0.5cm}
\begin{tabular}{p{3cm}p{12cm}}
    $R(m_1, m_2, z, \Omega)$ & the rate of signals from a direction in the sky ($\Omega$) at redshift $z$ with masses $m_1$, $m_2$, etc, measured with respect to detector-frame time. \\
    $dV/d\Omega dz$     & the differntial sensitive volume observed by our detectors as a function of time, position on the sky, and $z$, which depends on the detector sensitivity. \\
    $\mathcal{F}(m_1,m_2,z,\Omega;t)$ & the probability of detecting a signal with these parameters at this time (related to probabilities of specific noise realizations and the associated signal-to-noise ratios); in most cases this may be treated approximately as a step function.
\end{tabular}

\vspace{0.5cm}
\noindent
We also assume that reasonable priors for our Bayesian inference on a per-event basis follow the modeled astrophysical distribution so that
\begin{equation}
    \frac{p(m_1, m_2, z, \Omega|t,R)}{p(m_1^\prime, m_2^\prime, z^\prime, \Omega^\prime|t^\prime,R)} = \frac{R(m_1, m_2, z, \Omega) \left.(dV_c/d\Omega dz)\right|_{\Omega, z} \mathcal{F}(m_1, m_2, z, \Omega; t)}{R(m_1^\prime, m_2^\prime, z^\prime, \Omega^\prime) \left.(dV_c/d\Omega dz)\right|_{\Omega^\prime, z^\prime} \mathcal{F}(m_1^\prime, m_2^\prime, z^\prime, \Omega^\prime; t^\prime)}
\end{equation}
This means that the expected number of signals with masses $m_1$, $m_2$, etc. coming from $\Omega$ at $z$ coincident with $t$ will be something like 
\begin{equation}
    N_\mathrm{astro} = \left[ R(m_1, m_2, \Omega, z) \frac{dV_c}{d\Omega dz} \Delta\Omega \Delta z \Delta m_1 \Delta m_2 \right] \Delta t
\end{equation}
and we expect to detect 
\begin{equation}
    N_\mathrm{detect} = N_\mathrm{astro} \mathcal{F}(m_1, m_2, z, \Omega; t)
\end{equation}

Now, we consider rates that are low enough so that only one type of signal ($m_1$, $m_2$, $z$, $\Omega$) is detectable at a time and further model each type of signal as an independent Poisson process.
Approximating this as a set of discrete bins indexed by $k$, we expect
\begin{align}
    p(\mathrm{data}|\mathrm{signal}; R, t)p(\mathrm{signal}|R, t) & = \sum_k p(n_k=1|R_k dV_k \mathcal{F}_k \Delta t) \prod\limits_{j \neq k} p(n_j=0|R_j dV_j \mathcal{F}_j \Delta t) p\left(\mathrm{data}|m_1^{(k)}, m_2^{(k)}, \Omega^{(k)}, z^{(k)}\right) \nonumber \\
                                                                  & = \sum_k \left(R_k dV_k \mathcal{F}_k \Delta t\right) e^{-R_k dV_k \mathcal{F}_k \Delta t} \prod\limits_{j \neq k} e^{-R_j dV_j \mathcal{F}_j \Delta t} p\left(\mathrm{data}|m_1^{(k)}, m_2^{(k)}, \Omega^{(k)}, z^{(k)}\right) \nonumber \\
                                                                  & = \left[\sum_k \left(R_k dV_k \mathcal{F}_k \Delta t\right) p\left(\mathrm{data}|m_1^{(k)}, m_2^{(k)}, \Omega^{(k)}, z^{(k)}\right) \right] e^{-\sum\limits_j R_j dV_j \mathcal{F}_j \Delta t} \nonumber \\
                                                                  & = \Delta t \left[ \int dm_1 dm_2 dz d\Omega \left(R \frac{dV_c}{d\Omega dz} \mathcal{F}\right) p\left(\mathrm{data}|m_1, m_2, \Omega, z\right) \right] e^{-\left[\int dm_1 dm_2 dz d\Omega \left(R \frac{dV_c}{d\Omega dz} \mathcal{F}\right)\right] \Delta t}
\end{align}
and similarly
\begin{align}
    p(\mathrm{data}|\mathrm{noise}; R, t)p(\mathrm{noise}|R, t) & = p(\mathrm{data}|\mathrm{noise}) \prod\limits_j p(n_j=0|R_jdV_j \mathcal{F}_j \Delta t) \\
                                                                & = p(\mathrm{data}|\mathrm{noise}) e^{-\left[\int dm_1 dm_2 dz d\Omega \left(R \frac{dV_c}{d\Omega dz} \mathcal{F}\right)\right] \Delta t} \\
\end{align}
These define the relevant likelihoods for each small time segment.
We assume many time segments are independent and then stack everything together.
This give an overall joint likelihood of observing the set of detections we actually observed, conditioned on a Rate distribution: $R(m_1, m_2, z, \Omega)$.
Following Messenger+Veitch (2013), we obtain
\begin{align}
    p(\mathrm{data}|R) = & \prod\limits_{i|d_i\geq\mathrm{thr}} \left[ p(d_i|\mathrm{noise}) + \int dm_1 dm_2 dz d\Omega p(d_i|m_1, m_2, z, \Omega) R\frac{dV_c}{d\Omega dz} \mathcal{F}_i \Delta t \right] e^{-\int dm_1 dm_2 dz d\Omega R\frac{dV_c}{d\Omega dz} \mathcal{F}_i \Delta t} \nonumber \\
                         & \times \prod\limits_{j|d_j<\mathrm{thr}} \left[ p(d_i<\mathrm{thr}|\mathrm{noise}) + \int dm_1 dm_2 dz d\Omega p(d_i<\mathrm{thr}|m_1, m_2, z, \Omega) R\frac{dV_c}{d\Omega dz} \mathcal{F}_j \Delta t \right] e^{-\int dm_1 dm_2 dz d\Omega R\frac{dV_c}{d\Omega dz} \mathcal{F}_j \Delta t}
\end{align}
which we simplify by assuming
\begin{align}
    p(d_i<\mathrm{thr}|\mathrm{noise}) & = 1 \\
    p(d_i<\mathrm{thr}|m_1, m_2, z, \Omega)\mathcal{F}_i & = 0
\end{align}
This is equivalent to normalizing the likelihood appropriately to assume we only see signals above our threshold. 
In reality, we should be able to extract more information by lowering the threshold to include marginal events, but the associated benefits are expected to be realtively small with a significant computational overhead.
With our simplifying assumptions, we obtain
\begin{multline}
    \log p(\mathrm{data}|R) = - \int dt dm_1 dm_2 dz d\Omega \left(R \frac{dV_c}{d\Omega dz} \mathcal{F}\right) \\
         + \sum\limits_{i|d_i\geq\mathrm{thr}} \log \left[ \Delta t \int dm_1 dm_2 dz d\Omega \left(R \frac{dV_c}{d\Omega dz} \mathcal{F}\right) p(d_i|m_1, m_2, \Omega, z) \right] \\
         + \text{terms independent of }R
\end{multline}
This looks something like a Poisson factor for the overall rate along with the inner product of the evidence for each individual event.
We can use this marginal likelihood for the Rate distribution to make inferences about the rate.
Several approaches for sampling $p(R|\mathrm{data})$ have been proposed
\begin{itemize}
    \item bin the sky into separate voxels and run an MCMC direcly over the rate in each voxel. This can be expensive if there are a lot of voxels.
    \item assume the deviations from isotropy are small and attempt to infer the posterior of the deviations using Gaussian processes (see below).
\end{itemize}
along with several sanity checks and/or back-of-the-envelop estimates
\begin{itemize}
    \item divide the sky into hemispheres and count the posterior weight in each hemisphere as a statistic looking for a dipole.
    \item compute the 2-point auto-correlation function with stacked posteriors to look for preferred length scales.
\end{itemize}

%------------------------

\subsection*{Marginal likelihood for $R_\Omega$}

If we are only interested in isotropy contraints at the moment, we can assume a separation of the rate distribition: $R = p_{m,z}(m_1, m_2, z) R_\Omega(\Omega)$.
We have adopted a separation of variables in which the mass and redshift distribution integrates to one and the units are carried by the angular dependence.
Thus, adopting the convention
\begin{align}
    \mathcal{N} p_\Omega & = \int dm_1 dm_2 dz\, \left(p_{m,z} \frac{dV_c}{d\Omega dz} \mathcal{F}\right) \\
    \int d\Omega p_\Omega & = 1
\end{align}
so that
\begin{equation}
    \int dm_1 dm_2 dz \left( p_{m,z}\frac{dV_c}{d\Omega dz} \mathcal{F}\right) p(d_i|m_1, m_2, z, \Omega) = \mathcal{N} p(\Omega|d_i)
\end{equation}
we find
\begin{align}
    \log p(\mathrm{data}|R) \supset & - \int d\Omega\, R_\Omega \int dt \mathcal{N} p_\Omega + \sum\limits_{i|d_i\geq\mathrm{thr}} \log \left[ \Delta t \mathcal{N} \int d\Omega\, R_\Omega p(\Omega|d_i) \right] \\
                            \supset & - R_o \int d\Omega\, r_\Omega \int dt \mathcal{N} p_\Omega + \sum\limits_{i|d_i\geq\mathrm{thr}} \log \left[ \int d\Omega\, r_\Omega p(\Omega|d_i) \right] + \log\left[ R_o \mathcal{N} \Delta t \right]
\end{align}
where $R_\Omega = R_o r_\Omega$ and $R_o$ alone carries units.
We note that if we further require $\int d\Omega r_\Omega = 1$, $R_o$ sets the overall rate of signals detectable in our detectors.
This also shows that inference about $R_\Omega$ can be performed with knowledge of the a posteriori skymaps from each detection ($p(\Omega|d_i)$) and overall measures of the exposure as a function of $\Omega$ integrated over time ($\int dt \mathcal{N} p_\Omega$).

If we assume an isotropic distribution ($r_\Omega = 1/4\pi$), we obtain the expected inference for the overall rate of signals $R_o$.
In general, we will have to infer both $R_o$ and $r_\Omega$ simultaneously.


%------------------------

\subsection*{Analytic regression of small anisotropies}

Let us assume that $r_\Omega$ is nearly isotropic so that 
\begin{equation}
    \left. r_\Omega = \frac{1+\delta} \ \right| \ \delta \ll 1 \ \forall \ \Omega
\end{equation}
This yields
\begin{align}
    \log p(\mathrm{data}|R) \supset & - R_o \int dt \mathcal{N} + \sum\limits_{i|d_i\geq\mathrm{thr}} \left( \log\left[ R_o \mathcal{N} \Delta t \right] + 1 \right) \\
                                    & \quad \quad - R_o \int d\Omega\, \delta \int dt \mathcal{N} p_\Omega + \int d\Omega\, \delta \sum\limits_{i|d_i\geq\mathrm{thr}} p(\Omega|d_i) + O\left(\delta^2\right)
\end{align}
We note that, in this approximation, the log likelihood is linear in $\delta$ and therefore suitable for various analytic priors. 

As a purely pedagogical note, we examine the linear cofactor for $\delta$
\begin{equation}
    -R_o \int dt \mathcal{N} p_\Omega + \sum\limits_{i|d_i\geq\mathrm{thr}} p(\Omega|d_i)
\end{equation}
The first term is the Poisson factor accounting for how much exposure we have in any particular direction. 
All else being equal, if the exposure increases in a direction without more associated detections, the anisotropy is infered to be negative ($\delta < 0$). 
Similarly, if there is an excess of detections infered from a certain direction without an increased exposure, the anisotropy is infered to be positive ($\delta > 0$).
We also note that the intuitive notion of simply ``adding posteriors'' to look for excesses drops out of the linear approximaiton, although it is too naive for the more general inference.

One possible choice is a Gamma distribution for each $\Omega$ separately, which will yield an analytic Lomax posterior for each $\Omega$, related to the Pareto distribution.
Any correlations between neighboring $\Omega$ would come from the likelihood alone.

Another possible choice is a Guassian Process prior for $\delta$ assuming some covariance. 
A common choice might be the squared-exponential covariance kernel such that
\begin{equation}
    \left. \text{Cov}(\delta_1, \delta_2) = \sigma^2 \exp \left[ -\frac{\left(\Delta \theta\right)^2}{2l^2}\right] \quad  \right| \ \Delta \theta = \cos^{-1}\left(\cos\theta_1\cos\theta_2 + \sin\theta_1\sin\theta_2\cos(\phi_1-\phi_2)\right)
\end{equation}
If we wish to avoid introducing a length scale in our prior (i.e.: we're interested in measureing clustering), we can take the limit $l \rightarrow 0$ to obtain a ``white noise'' prior instead.
However, maximizing the associated likelihood as a function of $l$ may be a natural way to infer length scales in the data.

Assuming this prior and completing the square yields analytic inferences for $\delta$ at whatever resolution is desired along with error estimates.
Furthermore, because we assume $\delta \ll 1 \ \forall \ \Omega$, zero-mean Gaussian priors on $\delta$ are not unreasonable.
We also note that, in this approximation, the inferred posterior mean is given by the linear cofactor of $\delta$ in the likelihood.

%-------------------------------------------------
\end{document}
