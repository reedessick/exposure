#!/usr/bin/env python

__usage__ = "condor-compute_psd_horizon_sensitivity [--options] gpsstart gpsstop"
__description__ = "write a single DAG for the psd -> horizon -> sensitivity workflow"
__author__ = "Reed Essick (reed.essick@ligo.org)"

#-------------------------------------------------

import os
import getpass
import subprocess as sp
from collections import defaultdict

from optparse import OptionParser

### non-standard libraries
from exposure import utils
from exposure import datafind

from skymap_statistics import detector_cache

#-------------------------------------------------

parser = OptionParser(usage=__usage__, description=__description__)

parser.add_option('-v', '--verbose', default=False, action='store_true')
parser.add_option('-V', '--Verbose', default=False, action='store_true')

### general IFO specifications
parser.add_option('', '--ifo-specs', nargs=3, default=[], type='string', action='append',
    help='triples of IFO channel frametype. e.g.: "L L1:GDS-CALIB_STRAIN L1_HOFT_C00". \
can specify multiple detectors by repeating this option. \
DEFAULT=[]')

### options to figure out what data to pick up
parser.add_option("", "--include-flag", default=[], nargs=2, type='string', action='append',
    help='the ifo, flag pairs used to select subsets of [gpsstart, gpsstop] for analysis. \
Can be repeated to take the intersection of multiple flags, but intersections are taken for each IFO separately. \
DEFAULT=[] (analyze all time in [gpsstart, gpsstop]).')

parser.add_option("", "--exclude-flag", default=[], nargs=2, type='string', action='append',
    help='the same as --include-flag, except we only retain times that are \
outside of these flags instead of inside them')

parser.add_option("", "--joint-lock-only", default=False, action='store_true',
    help="if supplied, only analyze the intersection of all IFOs lock times. \
Can be used to remove any single-IFO times, etc.")

### options for how to compute PSD
parser.add_option("", "--win", default=60, type="int",
    help="estimate PSDs separately in sequential windows of this duration. \
DEFAULT=60")

parser.add_option("", "--seglen", default=4, type='int',
    help='the length of segments used to estimate the PSD via an averaging procedure (specify in seconds). \
NOTE: if we do not obtain an integer number of segments based on --seglen, --overlap, gpsstart, and gpsstop, \
we will raise a ValueError. DEFAULT=4')
parser.add_option("", "--overlap", default=2, type='float',
    help='the amount of time overlapped for segments used to estimate the PSD (specify in seconds). \
NOTE: if we do not obtain an integer number of segments based on --seglen, --overlap, gpsstart, and gpsstop, \
we will raise a ValueError. DEFAULT=2')
parser.add_option("", "--tukey-alpha", default=0.50, type='float',
    help='the Tukey "alpha" value used for windowing the DFT. \
DEFAULT=0.50')

### options for how to estimate horizon
parser.add_option('', '--flow', default=32, type='float',
    help='DEFAULT=32')
parser.add_option('', '--fhigh', default=1024, type='float',
    help='DEFAULT=1024')

parser.add_option('', '--distance', default=100, type='float',
    help='an arbitrary choice for the distance scale used to calculate the SNR and estimate the horizon')
parser.add_option('', '--snr-thr', default=8, type='float',
    help='an arbitrary snr threshold defining what is detectable. Used to estimate the horizon')

### options for network sensitivity
parser.add_option('', '--nside', default=512, type='int',
    help='the NSIDE resolution for the resulting FITS files')

### options for computing exposure
parser.add_option('', '--compute-exposure', default=False, action='store_true',
    help='add compute_exposure jobs to the DAG to stack everything together')

parser.add_option('', '--do-not-normalize', default=False, action='store_true',
    help='do not normalize total exposure so it sums to 1')

### options for condor
parser.add_option('', '--universe', default='vanilla', type='string',
    help='DEFAULT=vanilla')

parser.add_option('', '--psd-exe', default='compute_psd', type='string',
    help='specify the explicit path to the executable. \
DEFAULT=compute_psd')
parser.add_option('', '--horizon-exe', default='compute_horizon', type='string',
    help='specify the explicit path to the executable. \
DEFAULT=compute_horizon')
parser.add_option('', '--sensitivity-exe', default='compute_network_sensitivity', type='string',
    help='specify the explicit path to the executable. \
DEFAULT=compute_network_sensitivity')
parser.add_option('', '--exposure-exe', default='compute_exposure', type='string',
    help='specify the explicit path to the executable. \
DEFAULT=compute_exposure')

parser.add_option('', '--accounting-group', default='ligo.dev.o1.burst.explore.test', type='string',
    help='DEFAULT=ligo.dev.o1.burst.explore.test')
parser.add_option('', '--accounting-group-user', default=getpass.getuser(), type='string',
    help='DEFAULT='+getpass.getuser())

parser.add_option('', '--retry', default=0, type='int',
    help='DEFAULT=0')

parser.add_option('-s', '--condor-submit', default=False, action='store_true',
    help='submit the DAG to condor')

### options for output formatting
parser.add_option("-o", "--output-dir", default='.', type='string')
parser.add_option("-t", "--tag", default="", type='string')

opts, args = parser.parse_args()

assert len(args)==2, 'please supply exactly 2 arguments\n%s'%__usage__
gpsstart, gpsstop = [int(_) for _ in args]
stride = gpsstop - gpsstart

assert opts.seglen > opts.overlap, '--seglen must be larger than --overlap\n\%s'%__usage__

for ifo, channel, frametype in opts.ifo_specs:
    assert detector_cache.detectors.has_key(ifo), 'IFO=%s not known\n%s'%(ifo, __usage__)
    assert ifo==channel[0], 'I do not believe you want an IFO and channel \
from different instruments\n\tIFO : %s\n\tchannel : %s\n%s'%(ifo, channel, __usage__)
    assert ifo==frametype[0], 'I do not believe you want an IFO and frametype \
from different instruments\n\tIFO : %s\n\tframetype : %s\n%s'%(ifo, frametype, __usage__)

if opts.tag:
    filetag = "_"+opts.tag
else:
    filetag = ""

if not os.path.exists(opts.output_dir):
    os.makedirs(opts.output_dir)

opts.verbose |= opts.Verbose

#-----------

include_flag = defaultdict(list)
for ifo, flag in opts.include_flag:
    include_flag[ifo].append(flag)

exclude_flag = defaultdict(list)
for ifo, flag in opts.exclude_flag:
    exclude_flag[ifo].append(flag)

time_periods = defaultdict( dict )

#-------------------------------------------------

### set up DAG
dagname = "%s/compute_psd_horizon_sensitivity%s.dag"%(opts.output_dir, filetag)
if opts.verbose:
    print( "writing : "+dagname )
dag = open(dagname, 'w')

#------------------------

### figure out segments
segsdict = dict()
for ifo, channel, frametype in opts.ifo_specs:
    segsdict[ifo] = [[gpsstart, gpsstop]]
    segsdict[ifo] = datafind.include_flags(segsdict[ifo], include_flag[ifo], start, stride, verbose=opts.verbose)
    segsdict[ifo] = datafind.exclude_flags(segsdict[ifo], exclude_flag[ifo], start, stride, verbose=opts.verbose)
    
if opts.joint_lock_only:
    segments = [[gpsstart, gpsstop]]
    for ifo, _, _ in opts.ifo_specs:
        segments = utils.andsegments(segments, segsdict[ifo])

    for ifo, _, _ in opts.ifo_specs:
        segsdict[ifo] = segments

### figure out which PSDs will be available when
if opts.verbose:
    print( 'setting up PSD generation' )

for ifo, channel, frametype in opts.ifo_specs:

    ### check to make sure we have livetime left, etc
    assert segsdict[ifo], 'no remaining livetime after filtering by flags!'

    #-------

    subtag = '--tag '+opts.tag+ifo

    ### write PSD sub file
    psd_subname = "%s/compute_psd%s-%s.sub"%(opts.output_dir, filetag, ifo)
    if opts.verbose:
        print( "writing : "+psd_subname )
    sub = open(psd_subname, 'w')
    sub.write(utils.compute_psd_sub%{\
        'universe' : opts.universe,
        'exe' : opts.psd_exe,
        'channel' : channel,
        'frametype' : frametype,
        'accounting_group' : opts.accounting_group,
        'accounting_group_user' : opts.accounting_group_user,
        'tag' :  subtag,
        'filetag' : filetag,
        'start' : gpsstart,
        'dur' : gpsstop-gpsstart,
        'seglen' : opts.seglen,
        'overlap' : opts.overlap,
        'tukey_alpha' : opts.tukey_alpha,
    })
    sub.close()

    ### write horizon sub file
    hor_subname = "%s/compute_horizon%s-%s.sub"%(opts.output_dir, filetag, ifo)
    if opts.verbose:
        print( "writing : "+hor_subname )
    sub = open(hor_subname, 'w')
    sub.write(utils.compute_horizon_sub%{\
        'universe' : opts.universe,
        'exe' : opts.horizon_exe,
        'accounting_group' : opts.accounting_group,
        'accounting_group_user' : opts.accounting_group_user,
        'filetag' : filetag,
        'flow' : opts.flow,
        'fhigh' : opts.fhigh,
        'distance' : opts.distance,
        'snr_thr' : opts.snr_thr,
    })
    sub.close()

    ### iterate over segments and define compute_psd jobs for each
    covered = 0
    for segstart, segstop in segsdict[ifo]:
        segdur = segstop - segstart
        if opts.verbose:
            print( "scheduling jobs for %d -- %d"%(segstart, segstop) )

        s = (segstart/opts.win)*opts.win ### line-up start with integer number of windows. Needed to guarantee files will line up later -> integer division!
        if s < segstart: ### mostly likely case, but we need to check just in case
            s += opts.win

        while s+opts.win < segstop:
            outdir = utils.gps2dir(opts.output_dir, s, opts.win)
            if not os.path.exists(outdir):
                os.makedirs(outdir)

            ### add the PSD generation
            dag.write(utils.compute_psd_dag%{\
                'jobid' : '%s-%d'%(ifo, s),
                'sub' : psd_subname,
                'gpsstart' : s,
                'gpsstop' : s+opts.win,
                'retry' : opts.retry,
                'outdir' : outdir,
            })
            psd_path = utils.psd_path(outdir, '%s%s'%(filetag, ifo), s, opts.win) ### should produce this path

            ### add the horizon computation
            dag.write(utils.compute_horizon_dag%{\
                'jobid' : '%s-%d'%(ifo, s),
                'sub' : hor_subname,
                'path' : psd_path,
                'tag' : subtag,
                'retry' : opts.retry,
                'outdir' : outdir,
            })
            horizon_path = utils.horizon_path(outdir, '%s%s'%(filetag, ifo), s, opts.win) ### should produce this path

            ### add parent child relationships
            psd_jobid = 'compute_psd_%s-%d'%(ifo, s) ### FIXME: hard-coded jobid may be fragile...
            horizon_jobid = 'compute_horizon_%s-%d'%(ifo, s)
            dag.write('PARENT %s CHILD %s\n'%(psd_jobid, horizon_jobid))

            ### update book keeping
            time_periods[s].update({
                ifo:{
#                    'psd': psd_path,
#                    'psd_jobid' : psd_jobid,
                    'horizon' : horizon_path,
                    'horizon_jobid' : horizon_jobid,
                },
            })
            s += opts.win
            covered += opts.win

    if opts.Verbose: ### report amount of time covered
        print( 'requested       : %d sec'%(gpsstop-gpsstart) )
        print( 'within segments : %d sec'%sum([e-s for s, e in segsdict[ifo]]) )
        print( 'covered by PSD  : %d sec'%covered )

### add network sensitivity jobs
sen_subname = "%s/compute_network_sensitivity%s.sub"%(opts.output_dir, filetag)
if opts.verbose:
    print( "writing : "+sen_subname )
sen_sub = open(sen_subname, 'w')
sen_sub.write(utils.compute_network_sensitivity_sub%{\
    'universe' : opts.universe,
    'exe' : opts.sensitivity_exe,
    'accounting_group' : opts.accounting_group,
    'accounting_group_user' : opts.accounting_group_user,
    'nside' : opts.nside,
    'filetag' : filetag,
})
sen_sub.close()

if opts.compute_exposure: ### needed for book-keeping for how we structure the compute-exposure jobs
    long_periods = defaultdict( list )

for s, ifos in time_periods.items():
    outdir = utils.gps2dir(opts.output_dir, s, opts.win) ### should already exist!
    gps = s + 0.5*opts.win ### take the center of the window, which should be good enough...
    dag.write(utils.compute_network_sensitivity_dag%{\
        'jobid' : '%d'%s,
        'sub' : sen_subname,
        'gps' : gps, 
        'tag' : opts.tag,
        'ifo_horizons' : '--ifo-horizon '+' --ifo-horizon '.join('%s %s'%(ifo, paths['horizon']) for ifo, paths in ifos.items()),
        'retry' : opts.retry,
        'outdir': outdir,
    })

    sen_jobid = 'compute_network_sensitivity_%d'%s
    for ifo, paths in ifos.items():
        dag.write('PARENT %s CHILD %s\n'%(paths['horizon_jobid'], sen_jobid))

    if opts.compute_exposure: ### record jobid for future reference
        long_periods[s/100000].append({\
            'sen_jobid' : sen_jobid,
            'sen_path' : utils.sensitivity_path(outdir, filetag, gps, gzip=True),
        })

### add compute_exposure job
if opts.compute_exposure:

    exposures = [] ### list of all the smaller exposure jobs we gobble together
    for smod, paths in long_periods.items():
        ### write sub file
        outdir = os.path.join(opts.output_dir, '%d'%smod)
        exp_cachename = "%s/compute_exposure-%d.cache"%(outdir, smod)
        exp_subname = "%s/compute_exposure%s-%d.sub"%(opts.output_dir, filetag, smod)
        if opts.verbose:
            print( "writing : "+exp_subname )
        exp_sub = open(exp_subname, 'w')
        exp_sub.write(utils.compute_exposure_sub%{\
            'universe' : opts.universe,
            'exe' : opts.exposure_exe,
            'accounting_group' : opts.accounting_group,
            'accounting_group_user' : opts.accounting_group_user,
            'nside' : opts.nside,
            'index' : 3, ### weight by volume here
            'outdir' : outdir,
            'tag' : "--tag %d"%smod, 
            'filetag' : filetag,
        })
        exp_sub.close()

        ### add job to dag
        dag.write(utils.compute_exposure_dag%{\
            'jobid' : '%d'%smod,
            'sub' : exp_subname,
            'FITS' : '--cache '+exp_cachename,
            'normalize' : '--do-not-normalize',
            'retry' : opts.retry,
        })
        exp_jobid = 'compute_exposure_%d'%smod
        exp_path = '%s/exposure_%d.fits.gz'%(outdir, smod)

        ### add parent/child
        if opts.verbose:
            print( 'writing : '+exp_cachename )
        exp_cache = open(exp_cachename, 'w')
        for val in paths:
            dag.write('PARENT %s CHILD %s\n'%(val['sen_jobid'], exp_jobid))
            exp_cache.write('%s\n'%val['sen_path'])
        exp_cache.close()

        exposures.append({\
            'exp_jobid' : exp_jobid, 
            'exp_path' : exp_path,
        })

    ### add in final exposure job
    exp_cachename = "%s/compute_exposure%s.cache"%(opts.output_dir, filetag)
    exp_subname = "%s/compute_exposure%s.sub"%(opts.output_dir, filetag)
    if opts.verbose:
        print( "writing : "+exp_subname )
    exp_sub = open(exp_subname, 'w')
    exp_sub.write(utils.compute_exposure_sub%{\
        'universe' : opts.universe,
        'exe' : opts.exposure_exe,
        'accounting_group' : opts.accounting_group,
        'accounting_group_user' : opts.accounting_group_user,
        'nside' : opts.nside,
        'index' : 1, ### already weighted by volume, so just sum them here
        'outdir' : opts.output_dir,
        'tag' : '--tag '+opts.tag if opts.tag else '', 
        'filetag' : filetag,
    })
    exp_sub.close()

    ### add job to dag
    dag.write(utils.compute_exposure_dag%{\
        'jobid' : 'EVERYTHING',
        'sub' : exp_subname,
#        'FITS' : ' '.join(_['exp_path'] for _ in exposures),
        'FITS' : '--cache '+exp_cachename,
        'normalize' : '--do-not-normalize' if opts.do_not_normalize else '',
        'retry' : opts.retry,
    })
    exp_jobid = 'compute_exposure_EVERYTHING'

    ### add parent/child
    if opts.verbose:
        print( 'writing : '+exp_cachename )
    exp_cache = open(exp_cachename, 'w')
    for val in exposures:
        dag.write('PARENT %s CHILD %s\n'%(val['exp_jobid'], exp_jobid))
        exp_cache.write('%s\n'%val['exp_path'])
    exp_cache.close()

### done with the DAG
dag.close()

#-------------------------------------------------

### submit
if opts.condor_submit:
    if opts.verbose:
        print( 'submitting : '+dagname )
    sp.Popen(['condor_submit_dag', dagname]).wait()

elif opts.verbose:
    print( 'you can now submit : '+dagname )
